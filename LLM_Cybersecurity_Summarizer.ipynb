{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgI26BKAoztLhlBY/1j+a/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmuzquiz/LLM-Cybersecurity-Summarizer/blob/main/LLM_Cybersecurity_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Articles Used**"
      ],
      "metadata": {
        "id": "xk91zWV0AKlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#article 1\n",
        "#https://www.imf.org/external/pubs/ft/fandd/2021/03/global-cyber-threat-to-financial-systems-maurer.htm\n",
        "\n",
        "#article 2\n",
        "#https://news.vt.edu/articles/2024/08/it-cybersecurity-protections-enhanced-2-factor.html\n",
        "\n",
        "#article 3\n",
        "#https://www.ifac.org/knowledge-gateway/discussion/cybersecurity-critical-all-organizations-large-and-small\n",
        "\n",
        "#article 4\n",
        "#https://news.vt.edu/articles/2024/10/cci-cyberarts-2024-exhibit.html\n",
        "\n",
        "#article 5\n",
        "#https://www.propublica.org/article/cybersecurity-expert-finds-another-flaw-in-georgia-voter-portal"
      ],
      "metadata": {
        "id": "GTPYgUe-APQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install libraries**"
      ],
      "metadata": {
        "id": "AxHr9Exw_gX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TXWI5jSbBld",
        "outputId": "4b71d612-2ebc-418a-b55a-9e7546ca6827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (10.4.0)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=9fcfa6b1738eaab72275ec5cdd23fb0d56d8a3bbc4fcd9554c2866363817328b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=b0ac35e3900b7286891621226a3a6c255f4d894bca976a9124238032ebb62aab\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=e316a7bb8060576167c561c3a57dd51b5aaeabcfc93548c49b119261be39aca3\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=a48ce93f2e3377857440104e5a64f3419ee4e92325a6725594441cf06b81da59\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 transformers newspaper3k nltk\n",
        "#avg runtime 14 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beautiful Soup Method**"
      ],
      "metadata": {
        "id": "8gm8Kkpe_sG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "# requests: To download web content from the specified URL\n",
        "# BeautifulSoup: For parsing and extracting information from HTML content\n",
        "# transformers: To use a pre-trained model (BART) for text summarization\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "# Extract and clean article text from a given URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        # Send a GET request to the URL and raise an error for any bad response codes (e.g., 404)\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "\n",
        "        # Parse the HTML content of the article using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract the text content from all paragraph tags in the HTML document\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Clean up the extracted text by removing any extra spaces\n",
        "        article_text = ' '.join(article_text.split())  # Normalize spaces\n",
        "        return article_text.strip()  # Return the cleaned article text\n",
        "    except Exception as e:\n",
        "        # Handle errors that occur during the text extraction process\n",
        "        return f\"Failed to extract article text: {str(e)}\"\n",
        "\n",
        "# Summarize the text using a pre-trained transformer model\n",
        "def summarize_text(text):\n",
        "    # Initialize the pre-trained summarization model (BART Large CNN model)\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Split the input text into chunks of up to 800 characters, as the model has input size limitations\n",
        "    max_chunk_size = 800\n",
        "    text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    # Summarize each chunk and combine the resulting summaries into one\n",
        "    summaries = []\n",
        "    for chunk in text_chunks:\n",
        "        summary = summarizer(chunk, max_length=80, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    # Join the summaries and ensure the final result is clean and coherent\n",
        "    final_summary = ' '.join(summaries)\n",
        "    sentences = final_summary.split('.')\n",
        "    sentences = [s.strip() for s in sentences if s]\n",
        "\n",
        "    # Return a concise summary, limited to the first 5 sentences\n",
        "    final_summary = '.\\n'.join(sentences[:5])\n",
        "    # Safeguard to ensure each summary ends with a period\n",
        "    final_summary = final_summary + '.' if final_summary and not final_summary.endswith('.') else final_summary\n",
        "    return final_summary\n",
        "\n",
        "# Main execution flow\n",
        "if __name__ == \"__main__\":\n",
        "    # Prompt the user to enter the article URL\n",
        "    url = input(\"Enter Article URL: \")\n",
        "\n",
        "    # Extract the article text from the specified URL\n",
        "    article_text = extract_article_text(url)\n",
        "\n",
        "    # If the text extraction was successful, proceed to summarization\n",
        "    if not article_text.startswith(\"Failed\"):\n",
        "        summary = summarize_text(article_text)  # Summarize the extracted text\n",
        "        print(\"Summary:\")\n",
        "        print(summary)  # Display the final summary\n",
        "    else:\n",
        "        # Print the error message if extraction failed\n",
        "        print(article_text)\n"
      ],
      "metadata": {
        "id": "pOVWDY7QnXBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Newspaper3k Method**"
      ],
      "metadata": {
        "id": "wCf8zUx2_zAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "# nltk: For natural language processing tasks like sentence tokenization\n",
        "# Article: From the newspaper library, to easily handle web articles\n",
        "import nltk\n",
        "from newspaper import Article\n",
        "\n",
        "# Download the 'punkt' resource from nltk, used for sentence tokenization in NLP tasks\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract article information from a given URL\n",
        "def extract_article_info(url):\n",
        "    try:\n",
        "        # Create an Article object with the provided URL\n",
        "        article = Article(url)\n",
        "\n",
        "        # Download the article's HTML content\n",
        "        article.download()\n",
        "\n",
        "        # Parse the downloaded content to extract the article's text, title, authors, etc.\n",
        "        article.parse()\n",
        "\n",
        "        # Perform NLP tasks such as keyword extraction and summarization\n",
        "        article.nlp()\n",
        "\n",
        "        # Display key information about the article\n",
        "        print(f'Title: {article.title}')  # Print the title of the article\n",
        "        print(f'Authors: {article.authors}')  # Print the list of authors\n",
        "        print(f'Publication Date: {article.publish_date}')  # Print the publication date\n",
        "        print(f'Summary: {article.summary}')  # Print the summarized text of the article\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during article extraction and display the error message\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Main block of code to execute the program\n",
        "if __name__ == \"__main__\":\n",
        "    # Prompt the user to input the URL of the article they wish to extract\n",
        "    url = input(\"Enter Article URL: \")\n",
        "\n",
        "    # Call the function to extract and display the article's information\n",
        "    extract_article_info(url)\n"
      ],
      "metadata": {
        "id": "sssBxAn4nZ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other attempts**"
      ],
      "metadata": {
        "id": "EM66RGrJ__WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1st article soup\n",
        "#this code has an extra line to fix the first sentence from the first article I used\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract and clean article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraphs and join them\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Clean the text by removing extra spaces\n",
        "        article_text = ' '.join(article_text.split())\n",
        "        return article_text.strip()  # Return the cleaned text\n",
        "    except Exception as e:\n",
        "        return f\"Failed to extract article text: {str(e)}\"\n",
        "\n",
        "# Function to summarize text using a pre-trained transformer model\n",
        "def summarize_text(text):\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Break the text into chunks of a maximum of 800 characters for summarization\n",
        "    max_chunk_size = 800\n",
        "    text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    # Summarize each chunk and combine the results\n",
        "    summaries = []\n",
        "    for chunk in text_chunks:\n",
        "        # Summarize the chunk\n",
        "        summary = summarizer(chunk, max_length=80, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    # Join summaries into one final summary\n",
        "    final_summary = ' '.join(summaries)\n",
        "\n",
        "    # Refine the summary to get a coherent output with complete sentences\n",
        "    sentences = final_summary.split('.')\n",
        "\n",
        "    # Filter out any sentences that mention 'IMF Press Center' and strip extra spaces\n",
        "    sentences = [s.strip() for s in sentences if 'IMF Press Center' not in s and s] # This is the extra line\n",
        "\n",
        "    # Limit to 5 sentences for the final summary\n",
        "    final_summary = '.\\n'.join(sentences[:5])  # Use \\n for new line after each sentence\n",
        "\n",
        "    # Ensure each sentence ends with a period\n",
        "    final_summary = final_summary + '.' if final_summary and not final_summary.endswith('.') else final_summary\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")\n",
        "    article_text = extract_article_text(url)  # Extract the article text from the URL\n",
        "    if not article_text.startswith(\"Failed\"):  # If extraction is successful\n",
        "        summary = summarize_text(article_text)  # Summarize the extracted text\n",
        "        print(\"Summary:\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(article_text)  # Print error message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9b4f4kpdN-3",
        "outputId": "d5c33aa4-7383-4084-8247-c44ec6818729"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Article URL: https://www.imf.org/external/pubs/ft/fandd/2021/03/global-cyber-threat-to-financial-systems-maurer.htm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 80, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "Cyber threats to the financial system are growing, and the global community must cooperate to protect it.\n",
            "In February 2016, hackers targeted the central bank of Bangladesh and exploited vulnerabilities in SWIFT.\n",
            "The world’s governments and companies continue to struggle to contain the threat.\n",
            "It remains unclear who is responsible for protecting the system.\n",
            "The potential economic costs of such events can be immense and the damage to public trust and confidence significant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1st article news\n",
        "# Import the necessary libraries\n",
        "import nltk\n",
        "from newspaper import Article\n",
        "\n",
        "# Download the 'punkt' resource if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract article information\n",
        "def extract_article_info(url):\n",
        "    try:\n",
        "        article = Article(url)  # Create an Article object with the URL\n",
        "        article.download()      # Download the article\n",
        "        article.parse()         # Parse the article\n",
        "        article.nlp()           # Perform NLP on the article\n",
        "\n",
        "        # Print article information\n",
        "        print(f'Title: {article.title}')\n",
        "        print(f'Authors: {article.authors}')\n",
        "        print(f'Publication Date: {article.publish_date}')\n",
        "        print(f'Summary: {article.summary}')\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")  # Prompt for the article URL\n",
        "    extract_article_info(url)  # Extract and display the article information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T29WAgyte7QV",
        "outputId": "4ad8c9fe-6804-4000-d07e-2038d8d8916d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Article URL: https://www.imf.org/external/pubs/ft/fandd/2021/03/global-cyber-threat-to-financial-systems-maurer.htm\n",
            "Title: The Global Cyber Threat to Financial Systems – IMF F&D\n",
            "Authors: []\n",
            "Publication Date: None\n",
            "Summary: First, the global financial system is going through an unprecedented digital transformation, which is being accelerated by the COVID-19 pandemic.\n",
            "Second, malicious actors are taking advantage of this digital transformation and pose a growing threat to the global financial system, financial stability, and confidence in the integrity of the system.\n",
            "Although they do advance financial inclusion, digital financial services also offer a target-rich environment for hackers.\n",
            "Better protecting the global financial system is primarily an organizational challenge.\n",
            "This responsibility gap and continued uncertainty about roles and mandates to protect the global financial system fuel risks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2nd article\n",
        "#beautiful soup\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract and clean article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraphs and join them\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Clean the text by removing extra spaces\n",
        "        article_text = ' '.join(article_text.split())\n",
        "        return article_text.strip()  # Return the cleaned text\n",
        "    except Exception as e:\n",
        "        return f\"Failed to extract article text: {str(e)}\"\n",
        "\n",
        "# Function to summarize text using a pre-trained transformer model\n",
        "def summarize_text(text):\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Break the text into chunks of a maximum of 800 characters for summarization\n",
        "    max_chunk_size = 800\n",
        "    text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    # Summarize each chunk and combine the results\n",
        "    summaries = []\n",
        "    for chunk in text_chunks:\n",
        "        # Summarize the chunk\n",
        "        summary = summarizer(chunk, max_length=80, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    # Join summaries into one final summary\n",
        "    final_summary = ' '.join(summaries)\n",
        "\n",
        "    # Refine the summary to get a coherent output with complete sentences\n",
        "    sentences = final_summary.split('.')\n",
        "\n",
        "    # Strip extra spaces from each sentence\n",
        "    sentences = [s.strip() for s in sentences if s]\n",
        "\n",
        "    # Limit to 5 sentences for the final summary\n",
        "    final_summary = '.\\n'.join(sentences[:5])  # Use \\n for new line after each sentence\n",
        "\n",
        "    # Ensure each sentence ends with a period\n",
        "    final_summary = final_summary + '.' if final_summary and not final_summary.endswith('.') else final_summary\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")\n",
        "    article_text = extract_article_text(url)  # Extract the article text from the URL\n",
        "    if not article_text.startswith(\"Failed\"):  # If extraction is successful\n",
        "        summary = summarize_text(article_text)  # Summarize the extracted text\n",
        "        print(\"Summary:\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(article_text)  # Print error message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92b9ziksfPsx",
        "outputId": "b73c2a65-bfc8-4cae-fabc-b0dd2faf70d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Article URL: https://news.vt.edu/articles/2024/08/it-cybersecurity-protections-enhanced-2-factor.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 80, but your input_length is only 11. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "A wave of phishing emails targeting Virginia Tech employees attempted to diverting direct deposits, including pay, away from their legitimate destination.\n",
            "Fortunately, newly deployed cybersecurity protections within the Division of Information Technology detected the unusual login activity and put a stop to the hack.\n",
            "Hackers are getting better at what they do, and they are studying our business processes to find vulnerabilities.\n",
            "Each member of the university community has a role to play in staying safe online.\n",
            "\"We must continue to find ways to shore up cyber defenses, to include a more informed and security-aware community,\" he says.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2nd newspaper\n",
        "\n",
        "# Import the necessary libraries\n",
        "import nltk\n",
        "from newspaper import Article\n",
        "\n",
        "# Download the 'punkt' resource if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract article information\n",
        "def extract_article_info(url):\n",
        "    try:\n",
        "        article = Article(url)  # Create an Article object with the URL\n",
        "        article.download()      # Download the article\n",
        "        article.parse()         # Parse the article\n",
        "        article.nlp()           # Perform NLP on the article\n",
        "\n",
        "        # Print article information\n",
        "        print(f'Title: {article.title}')\n",
        "        print(f'Authors: {article.authors}')\n",
        "        print(f'Publication Date: {article.publish_date}')\n",
        "        print(f'Summary: {article.summary}')\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")  # Prompt for the article URL\n",
        "    extract_article_info(url)  # Extract and display the article information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxU8_Q7wfwM1",
        "outputId": "a793dee8-242f-46ab-e01d-bacbaa388608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Article URL: https://news.vt.edu/articles/2024/08/it-cybersecurity-protections-enhanced-2-factor.html\n",
            "An error occurred: Article `download()` failed with 403 Client Error: Forbidden for url: https://news.vt.edu/articles/2024/08/it-cybersecurity-protections-enhanced-2-factor.html on URL https://news.vt.edu/articles/2024/08/it-cybersecurity-protections-enhanced-2-factor.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3rd article\n",
        "#beautiful soup\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract and clean article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraphs and join them\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Clean the text by removing extra spaces\n",
        "        article_text = ' '.join(article_text.split())\n",
        "        return article_text.strip()  # Return the cleaned text\n",
        "    except Exception as e:\n",
        "        return f\"Failed to extract article text: {str(e)}\"\n",
        "\n",
        "# Function to summarize text using a pre-trained transformer model\n",
        "def summarize_text(text):\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Break the text into chunks of a maximum of 800 characters for summarization\n",
        "    max_chunk_size = 800\n",
        "    text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    # Summarize each chunk and combine the results\n",
        "    summaries = []\n",
        "    for chunk in text_chunks:\n",
        "        # Summarize the chunk\n",
        "        summary = summarizer(chunk, max_length=80, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    # Join summaries into one final summary\n",
        "    final_summary = ' '.join(summaries)\n",
        "\n",
        "    # Refine the summary to get a coherent output with complete sentences\n",
        "    sentences = final_summary.split('.')\n",
        "\n",
        "    # Strip extra spaces from each sentence\n",
        "    sentences = [s.strip() for s in sentences if s]\n",
        "\n",
        "    # Limit to 5 sentences for the final summary\n",
        "    final_summary = '.\\n'.join(sentences[:5])  # Use \\n for new line after each sentence\n",
        "\n",
        "    # Ensure each sentence ends with a period\n",
        "    final_summary = final_summary + '.' if final_summary and not final_summary.endswith('.') else final_summary\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")\n",
        "    article_text = extract_article_text(url)  # Extract the article text from the URL\n",
        "    if not article_text.startswith(\"Failed\"):  # If extraction is successful\n",
        "        summary = summarize_text(article_text)  # Summarize the extracted text\n",
        "        print(\"Summary:\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(article_text)  # Print error message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P0bRMlMf-yW",
        "outputId": "5eb4766e-fe39-4149-e894-774c082aa670"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Article URL: https://www.ifac.org/knowledge-gateway/discussion/cybersecurity-critical-all-organizations-large-and-small\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 80, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "Cybercrime is becoming big business and cyber risk a focus of organizations and governments globally.\n",
            "Monetary and reputational risks are high if organizations don’t have an appropriate cybersecurity plan.\n",
            "Cyber-attacks have been steadily climbing for four consecutive years.\n",
            "The manufacturing sector experienced the greatest proportion of cyber-attacks in 2022.\n",
            "Recent cases have involved thefts of sensitive information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3rd newspaper\n",
        "\n",
        "# Import the necessary libraries\n",
        "import nltk\n",
        "from newspaper import Article\n",
        "\n",
        "# Download the 'punkt' resource if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract article information\n",
        "def extract_article_info(url):\n",
        "    try:\n",
        "        article = Article(url)  # Create an Article object with the URL\n",
        "        article.download()      # Download the article\n",
        "        article.parse()         # Parse the article\n",
        "        article.nlp()           # Perform NLP on the article\n",
        "\n",
        "        # Print article information\n",
        "        print(f'Title: {article.title}')\n",
        "        print(f'Authors: {article.authors}')\n",
        "        print(f'Publication Date: {article.publish_date}')\n",
        "        print(f'Summary: {article.summary}')\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")  # Prompt for the article URL\n",
        "    extract_article_info(url)  # Extract and display the article information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaFpln-khL0a",
        "outputId": "b7427947-6eee-4aa0-b65f-5aad67708732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Article URL: https://www.ifac.org/knowledge-gateway/discussion/cybersecurity-critical-all-organizations-large-and-small\n",
            "Title: Cybersecurity Is Critical for all Organizations – Large and Small\n",
            "Authors: []\n",
            "Publication Date: None\n",
            "Summary: Cybersecurity is making sure your organization's data is safe from attacks from both internal and external bad actors.\n",
            "Once infected, the organization’s data continues to be inaccessible as the encrypts the data using the attackers encryption key.\n",
            "Cybersecurity GovernanceA cybersecurity governance and risk management program should be established which is appropriate for the size of the organization.\n",
            "Cybersecurity risk needs to be considered as a significant business risk by the owners and directors.\n",
            "Reporting of any possible breach of security, unauthorized access, or disclosure of the organizations data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4th article\n",
        "#beautiful soup\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract and clean article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraphs and join them\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Clean the text by removing extra spaces\n",
        "        article_text = ' '.join(article_text.split())\n",
        "        return article_text.strip()  # Return the cleaned text\n",
        "    except Exception as e:\n",
        "        return f\"Failed to extract article text: {str(e)}\"\n",
        "\n",
        "# Function to summarize text using a pre-trained transformer model\n",
        "def summarize_text(text):\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Break the text into chunks of a maximum of 800 characters for summarization\n",
        "    max_chunk_size = 800\n",
        "    text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    # Summarize each chunk and combine the results\n",
        "    summaries = []\n",
        "    for chunk in text_chunks:\n",
        "        # Summarize the chunk\n",
        "        summary = summarizer(chunk, max_length=80, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    # Join summaries into one final summary\n",
        "    final_summary = ' '.join(summaries)\n",
        "\n",
        "    # Refine the summary to get a coherent output with complete sentences\n",
        "    sentences = final_summary.split('.')\n",
        "\n",
        "    # Strip extra spaces from each sentence\n",
        "    sentences = [s.strip() for s in sentences if s]\n",
        "\n",
        "    # Limit to 5 sentences for the final summary\n",
        "    final_summary = '.\\n'.join(sentences[:5])  # Use \\n for new line after each sentence\n",
        "\n",
        "    # Ensure each sentence ends with a period\n",
        "    final_summary = final_summary + '.' if final_summary and not final_summary.endswith('.') else final_summary\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")\n",
        "    article_text = extract_article_text(url)  # Extract the article text from the URL\n",
        "    if not article_text.startswith(\"Failed\"):  # If extraction is successful\n",
        "        summary = summarize_text(article_text)  # Summarize the extracted text\n",
        "        print(\"Summary:\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(article_text)  # Print error message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFelyqYYgE3w",
        "outputId": "0a53a7a3-edef-46d7-d38b-dd657a1ede92"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Article URL: https://news.vt.edu/articles/2024/10/cci-cyberarts-2024-exhibit.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 80, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "CyberArts 2024 opens at the Torpedo Factory Art Center in Alexandria.\n",
            "The opening reception will be held on Oct.\n",
            "18 from 6-8 p.\n",
            "m.\n",
            "Registration is required.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4th newspaper\n",
        "\n",
        "# Import the necessary libraries\n",
        "import nltk\n",
        "from newspaper import Article\n",
        "\n",
        "# Download the 'punkt' resource if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract article information\n",
        "def extract_article_info(url):\n",
        "    try:\n",
        "        article = Article(url)  # Create an Article object with the URL\n",
        "        article.download()      # Download the article\n",
        "        article.parse()         # Parse the article\n",
        "        article.nlp()           # Perform NLP on the article\n",
        "\n",
        "        # Print article information\n",
        "        print(f'Title: {article.title}')\n",
        "        print(f'Authors: {article.authors}')\n",
        "        print(f'Publication Date: {article.publish_date}')\n",
        "        print(f'Summary: {article.summary}')\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")  # Prompt for the article URL\n",
        "    extract_article_info(url)  # Extract and display the article information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3X79MLQhSwM",
        "outputId": "42eeab06-78dd-45b3-8018-a744e4151aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Article URL: https://news.vt.edu/articles/2024/10/cci-cyberarts-2024-exhibit.html\n",
            "An error occurred: Article `download()` failed with 403 Client Error: Forbidden for url: https://news.vt.edu/articles/2024/10/cci-cyberarts-2024-exhibit.html on URL https://news.vt.edu/articles/2024/10/cci-cyberarts-2024-exhibit.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5th article\n",
        "#beautiful soup\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract and clean article text from a URL\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraphs and join them\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        # Clean the text by removing extra spaces\n",
        "        article_text = ' '.join(article_text.split())\n",
        "        return article_text.strip()  # Return the cleaned text\n",
        "    except Exception as e:\n",
        "        return f\"Failed to extract article text: {str(e)}\"\n",
        "\n",
        "# Function to summarize text using a pre-trained transformer model\n",
        "def summarize_text(text):\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Break the text into chunks of a maximum of 800 characters for summarization\n",
        "    max_chunk_size = 800\n",
        "    text_chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]\n",
        "\n",
        "    # Summarize each chunk and combine the results\n",
        "    summaries = []\n",
        "    for chunk in text_chunks:\n",
        "        # Summarize the chunk\n",
        "        summary = summarizer(chunk, max_length=80, min_length=30, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    # Join summaries into one final summary\n",
        "    final_summary = ' '.join(summaries)\n",
        "\n",
        "    # Refine the summary to get a coherent output with complete sentences\n",
        "    sentences = final_summary.split('.')\n",
        "\n",
        "    # Strip extra spaces from each sentence\n",
        "    sentences = [s.strip() for s in sentences if s]\n",
        "\n",
        "    # Limit to 5 sentences for the final summary\n",
        "    final_summary = '.\\n'.join(sentences[:5])  # Use \\n for new line after each sentence\n",
        "\n",
        "    # Ensure each sentence ends with a period\n",
        "    final_summary = final_summary + '.' if final_summary and not final_summary.endswith('.') else final_summary\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")\n",
        "    article_text = extract_article_text(url)  # Extract the article text from the URL\n",
        "    if not article_text.startswith(\"Failed\"):  # If extraction is successful\n",
        "        summary = summarize_text(article_text)  # Summarize the extracted text\n",
        "        print(\"Summary:\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(article_text)  # Print error message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hkr64L-hC-8",
        "outputId": "65a2237e-9619-48d2-9f3a-a782c231adbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Article URL: https://www.propublica.org/article/cybersecurity-expert-finds-another-flaw-in-georgia-voter-portal\n",
            "Summary:\n",
            "Until Monday, a new online portal run by the Georgia Secretary of State’s Office contained what experts describe as a serious security vulnerability.\n",
            "The flaw was brought to the attention of ProPublica and Atlanta News First over the weekend.\n",
            "The issue was “as bad as any voter cancellation bug could be,” a cybersecurity researcher says.\n",
            "The Georgia Secretary of State’s Office said it had no records of Parker's attempts to reach out.\n",
            "The Secretary of State’s Office told the news organizations that it quickly fixed the portal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5th newspaper\n",
        "\n",
        "# Import the necessary libraries\n",
        "import nltk\n",
        "from newspaper import Article\n",
        "\n",
        "# Download the 'punkt' resource if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract article information\n",
        "def extract_article_info(url):\n",
        "    try:\n",
        "        article = Article(url)  # Create an Article object with the URL\n",
        "        article.download()      # Download the article\n",
        "        article.parse()         # Parse the article\n",
        "        article.nlp()           # Perform NLP on the article\n",
        "\n",
        "        # Print article information\n",
        "        print(f'Title: {article.title}')\n",
        "        print(f'Authors: {article.authors}')\n",
        "        print(f'Publication Date: {article.publish_date}')\n",
        "        print(f'Summary: {article.summary}')\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter Article URL: \")  # Prompt for the article URL\n",
        "    extract_article_info(url)  # Extract and display the article information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ZxCZD0hChk",
        "outputId": "a74537c4-fb65-4631-9f43-9a7503b41a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Article URL: https://www.propublica.org/article/cybersecurity-expert-finds-another-flaw-in-georgia-voter-portal\n",
            "Title: “A Terrible Vulnerability”: Cybersecurity Researcher Discovers Yet Another Flaw in Georgia’s Voter Cancellation Portal\n",
            "Authors: ['Doug Bock Clark', 'Doug Bock Clark Is A Reporter In Propublica S South Unit. He Investigates Threats To Democracy', 'Abuses Of Power Throughout The Region.']\n",
            "Publication Date: None\n",
            "Summary: Parker, who uses they/them pronouns, said that after discovering it, they attempted to contact the Georgia Secretary of State’s Office.\n",
            "The Secretary of State’s Office told the news organizations that it quickly fixed the portal.\n",
            "This one would allow any user of the portal to bypass the screen that requires a driver’s license number and submit the cancellation request without it.\n",
            "A window popped up stating that “Your cancellation request has been successfully submitted” and that county election workers would process the request within a week.\n",
            "(Parker’s cancellation request would have lacked a driver’s license number.)\n"
          ]
        }
      ]
    }
  ]
}